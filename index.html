<h3>Transfer Learning for NLP</h3>
<br>
Transferring knowledge from related domains, tasks, and languages to the target setting.<br>

Where transfer learning will not work -<br>
Enough data is already available.<br>
No relevant info is available.<br>
<br>
<h4>Prerequisite -</h4>
<b>Probability</b> - Random Variable, PMF and PDF, Joint, marginal, and conditional probability distribution, Baye's Rule, Expectation, Variance, Standard deviation, Pearson correlation coefficient, Covariance matrix, Cannonical correlation analysis (CCA)<br>
<b>Distributions</b> - Bernoulli distribution, Categorical or Multinoulli distribution, Gaussian or normal distribution, Empirical distribution.<br>
<b>Information Theory</b> - Self information, Shannon entropy, Kullback-Leibler divergence (KL divergence) or information gain, Jenson-Shannon (JS) divergence, cross-entropy, Pointwise Mutual Information (PMI), Mutual Information (MI)<br>
 
